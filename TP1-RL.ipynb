{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif : ### \n",
    "#### L'objectif de ce TP est de se familiariser avec les outils essentiels du Reinforcement Learning (RL), notamment OpenAI Gym. Les étudiants vont explorer comment interagir avec un environnement RL et exécuter des actions avant d'implémenter un algorithme d'apprentissage dans les séances suivantes. #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 1 : Présentation des Bibliothèques Clés ###\n",
    "### Création d'un environnement ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, _ = env.reset() # Obtenir l'etat initial de l'environnement \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2: Exercices Pratiques avec OpenAI Gym ###\n",
    "### Exercice 1: Découverte et Exploration d'un Environnement Gym ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions : Discrete(2)\n",
      "Espace d'observations : Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action: 1, Observation: [-0.00778038  0.23590222 -0.02209515 -0.2920953 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00306234  0.4313321  -0.02793705 -0.5916641 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.0055643   0.6268338  -0.03977033 -0.8930146 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01810098  0.822472   -0.05763062 -1.1979291 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03455042  0.6281412  -0.08158921 -0.923851  ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.04711324  0.82426494 -0.10006623 -1.2410185 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.06359854  0.63055986 -0.1248866  -0.98128486], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.07620974  0.43731278 -0.1445123  -0.7302931 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.084956    0.63410497 -0.15911816 -1.0647433 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.09763809  0.83093375 -0.18041302 -1.4028404 ], Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Espace d'actions : {env.action_space}\")\n",
    "print(f\"Espace d'observations : {env.observation_space}\")\n",
    "\n",
    "for _ in range(10):\n",
    "    actions = env.action_space.sample() # Action aléatoire\n",
    "    observation, reward, done, info, _ = env.step(actions) # Execute les actions et récupere les resultats \n",
    "    \n",
    "    print(f\"Action: {actions}, Observation: {observation}, Reward: {reward}\")\n",
    "    \n",
    "    if done:\n",
    "        observation, _ = env.reset()  \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Manipulation des Observations et Récompenses ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Observation: [-0.02030969 -0.19467616  0.00120108  0.32436326], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02420321 -0.38981518  0.00768834  0.6174247 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.03199952 -0.5850437   0.02003684  0.91251916], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04370039 -0.7804309   0.03828722  1.2114316 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.05930901 -0.5858236   0.06251585  0.93098825], Reward: 1.0\n",
      "Action: 0, Observation: [-0.07102548 -0.781731    0.08113562  1.242643  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.0866601  -0.9777951   0.10598848  1.5595994 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.10621601 -0.7840894   0.13718046  1.3017737 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.12189779 -0.98065907  0.16321594  1.6340628 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.14151098 -0.7877859   0.19589719  1.3963706 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.15726669 -0.9847297   0.2238246   1.7433622 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.17696129 -1.1815877   0.25869185  2.0965397 ], Reward: 0.0\n",
      "Action: 0, Observation: [-0.20059304 -1.3782178   0.30062264  2.4568808 ], Reward: 0.0\n",
      "Action: 0, Observation: [-0.22815739 -1.5743935   0.34976026  2.825005  ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.25964525 -1.382866    0.40626037  2.6558537 ], Reward: 0.0\n",
      "Action: 0, Observation: [-0.28730258 -1.5782422   0.45937744  3.041246  ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.31886742 -1.3875618   0.52020234  2.9152343 ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.34661865 -1.1979322   0.57850707  2.814551  ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.3705773  -1.009277    0.63479805  2.7383668 ], Reward: 0.0\n",
      "Action: 1, Observation: [-0.39076284 -0.82149655  0.6895654   2.6859145 ], Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, _ = env.reset()\n",
    "for i in range(20):\n",
    "    actions = env.action_space.sample() # choisit une action aleatoire.\n",
    "    observation,reward , done , info , _ = env.step(actions) # retourne ,l'action l'observation , la récompense\n",
    "    print(f\"Action: {actions}, Observation: {observation}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observation : L'état actuel du système (position et vitesse du chariot, angle et vitesse de la tige).\n",
    "- Recompense : 1 si l'épisode continue, 0 si l'épisode est terminé (le pôle est tombé).\n",
    "- Les observations changent a chaque action en fonction de l'état du systeme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Contrôle Manuel de l'Agent ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [ 0.00531586 -0.18011384  0.00089797  0.3214699 ], Récompense: 1.0\n",
      "Observation: [ 0.00171358 -0.37524855  0.00732737  0.61443585], Récompense: 1.0\n",
      "Observation: [-0.00579139 -0.5704721   0.01961609  0.90941757], Récompense: 1.0\n",
      "Observation: [-0.01720083 -0.765854    0.03780444  1.2082008 ], Récompense: 1.0\n",
      "Observation: [-0.03251791 -0.96144336  0.06196846  1.512487  ], Récompense: 1.0\n",
      "Observation: [-0.05174678 -1.1572585   0.0922182   1.8238531 ], Récompense: 1.0\n",
      "Observation: [-0.07489195 -1.3532751   0.12869526  2.1437023 ], Récompense: 1.0\n",
      "Observation: [-0.10195746 -1.5494106   0.1715693   2.4732046 ], Récompense: 1.0\n",
      "Observation: [-0.13294567 -1.7455081   0.2210334   2.8132265 ], Récompense: 1.0\n",
      "Durée totale de l'épisode : 9 étapes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "observation, _ = env.reset() # renitialise et recupere etat init de env\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    action = int(input(\"Entrez une action (0 ou 1) : \"))  \n",
    "    if action not in [0, 1]:\n",
    "        print(\"Action invalide, veuillez entrer 0 ou 1.\")\n",
    "        continue\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action) # execute action et recupere\n",
    "    print(f\"Observation: {observation}, Récompense: {reward}\") #observation et reward apres action\n",
    "\n",
    "    steps += 1\n",
    "    done = terminated or truncated  # truncated limite temps\n",
    "\n",
    "print(f\"Durée totale de l'épisode : {steps} étapes\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saisie de l’action : L'utilisateur entre une action (0 ou 1), avec vérification de validité.\n",
    "- Exécution de l’action : L'action est effectuée, et les nouvelles observations et récompenses sont affichées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée moyenne d'un épisode après 10 épisodes : 22.5 étapes\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "num_episodes = 10  # Nombre d'épisodes à exécuter\n",
    "total_steps = 0\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    observation, _ = env.reset()  # Reinitialise l'environnement et récupere l'observation initiale\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    # Exécuter un épisode avec des actions aléatoires\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Action aleatoire\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)  # Execute l'action\n",
    "        steps += 1\n",
    "        done = terminated or truncated  # Verifie la fin de l'épisode\n",
    "\n",
    "    total_steps += steps  # Additionner le nombre d'étapes de chaque épisode\n",
    "\n",
    "# Calcul de la durée moyenne\n",
    "average_steps = total_steps / num_episodes\n",
    "print(f\"Durée moyenne d'un épisode après {num_episodes} épisodes : {average_steps} étapes\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
